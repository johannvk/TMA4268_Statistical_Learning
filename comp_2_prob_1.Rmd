---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 31"
author: "Ingrid Sofie Skjetne and Johannes Voll Kolst√∏"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  #html_document
  pdf_document
   
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r rpackages,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")
#install.packages("corrplot")
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(leaps)
library(glmnet)
library(corrplot)

set.seed(1)
```

# Problem 1

## a)

The ridge estimator is the value $\hat{\beta}_{R}$ of $\beta$ that minimizes  
\begin{align*}
  \text{RSS} + \lambda \sum_{j=1}^p \beta_j^2 &= (y-X\beta)^\text{T}(y-X\beta) + \lambda \beta^\text{T}\beta \\
  &= (y^\text{T} - \beta^\text{T} X^\text{T})(y-X \beta) + \lambda \beta^\text{T}\beta\\
  &= (y^\text{T} - \beta^\text{T} X^\text{T})y - (y^\text{T} - \beta^\text{T} X^\text{T}) X\beta + \lambda \beta^\text{T}\beta\\
  &= y^\text{T}y -\beta^\text{T}X^\text{T}y-y^\text{T}X\beta + \beta^\text{T}X^\text{T}X\beta +\lambda \beta^\text{T}\beta.\\
\end{align*}

The value above is the $\text{RSS}$ plus a term which penalizes large values of $\beta$, which forces the parameter $\beta$ to shrink. In order to find the minimizer $\hat{\beta}_{R}$, we set the partial derivative of the above expression with respect to $\beta$ equal to zero. The partial derivative with respect to $\beta$ is 
\begin{align*}
&\frac{\partial}{\partial \beta} \left(  (y-X\beta)^\text{T}(y-X\beta) + \lambda \beta^\text{T}\beta  \right) \\
=& \frac{\partial}{\partial \beta} \left(  y^\text{T}y -\beta^\text{T}X^\text{T}y-y^\text{T}X\beta + \beta^\text{T}X^\text{T}X\beta +\lambda \beta^\text{T}\beta  \right) \\
=& -2X^\text{T}y+2X^\text{T}X\beta +2\lambda I.
\end{align*}

Setting the partial derivative equal to zero in order to find the minimizer yields
\begin{align*}
-2X^\text{T}y+2X^\text{T}X\hat{\beta}_{R} +2\lambda I \hat{\beta}_{R} &= 0\\
\implies -X^\text{T}y+X^\text{T}X\hat{\beta}_{R} +\lambda I\hat{\beta}_{R} &= 0 \\
\implies X^\text{T}X\hat{\beta}_{R} +\lambda I\hat{\beta}_{R} &= X^\text{T}y \\
\implies \hat{\beta}_{R} &= (X^\text{T}X +\lambda I)^{-1}X^\text{T}y.
\end{align*}


## b)

The expected value of $\hat{\beta}_{R}$ is 
\begin{align*}
(X^\text{T}X +\lambda I)^{-1}X^\text{T}X\beta.
\end{align*}


The covariance matrix of the ridge estimator is 
\begin{align*}
&(X^\text{T}X +\lambda I)^{-1}X^\text{T} \text{Var}(Y) ((X^\text{T}X +\lambda I)^{-1}X^\text{T})^\text{T} \\
=& (X^\text{T}X +\lambda I)^{-1}X^\text{T} \sigma^2 I ((X^\text{T}X +\lambda I)^{-1}X^\text{T})^\text{T} \\
=& \sigma^2 (X^\text{T}X +\lambda I)^{-1}X^\text{T}  X((X^\text{T}X +\lambda I)^{-1})^\text{T}. \\
\end{align*}


## c)

i. True.
ii. False.
iii. False.
iv. True. 


## d)

We split the `College` dataset into training data and test data.
```{r}
train.ind = sample(1:nrow(College), 0.5*nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
```

Next, we apply forward stepwise selection to choose a model using BIC. BIC places a higher penalty on complexity than AIC and $C_p$. This might be appropriate in this case, as it is desirable to have an interpretable model which can shed some light on which predictors are important, not only make predictions using the model. 

```{r, fig.height=5, fig.width=6, fig.show="hold"}
outstate_forw_fit = regsubsets(Outstate ~ . , nvmax = ncol(College)-1, data=College, method="forward")
outstate_summary = summary(outstate_forw_fit)
plot(outstate_summary$bic, type="l", xlab="Number of variables", ylab="BIC")
axis(side=1, at= c(1:17))
which.min(outstate_summary$bic)
```

Looking at the plot displaying $\text{BIC}$, we can see that model 13 has the lowest $\text{BIC}$. We extract the coefficients of model 13.
```{r}
coef(outstate_forw_fit, 13)
```
The model is 
$$
\begin{aligned}
\texttt{Outstate} = &-1892.08 + 2265.83 \cdot \texttt{PrivateYes} - 0.28 \cdot \texttt{Apps} + 0.72 \cdot \texttt{Accept} + 22.46 \cdot \texttt{Top10perc} - 0.168 \cdot \texttt{F.Undergrad} + 0.89 \cdot \texttt{Room.Board}-0.25 \cdot \texttt{Personal} \\
&+12.81 \cdot \texttt{PhD} + 23.65 \cdot \texttt{Terminal} -46.94 \cdot \texttt{S.F.Ratio} - 41.01\cdot\texttt{perc.alumni} + 0.20 \cdot \texttt{Expend} + 23.63\cdot \texttt{Grad.Rate}.
\end{aligned}
$$

To find the MSE on the test set, we first fit a linear model using the variables in the chosen model.

```{r}
outstate_model_13 = lm(data=College, Outstate ~ Private + Apps + Accept + Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate)
```

Now, out of state tuition for the colleges in the test set is predicted using the model.

```{r}
outstate_test_real = college.test[9]
predicted_outstate = predict(outstate_model_13, college.test[-9])
```

Calculating the test mean squared error.
```{r}
residuals = outstate_test_real - predicted_outstate
mse_forw = mean(as.matrix(residuals^2))
```
The mean squared error on the test set is `r mse_forw`. 

## e)

Now we will use the Lasso method to perform model selection using the `College` dataset. First we convert the training and test data into matrix format.
```{r}
college_predictors_train = data.matrix(college.train[-9])
college_response_train = data.matrix(college.train[9])

college_predictors_test = data.matrix(college.test[-9])
college_response_test = data.matrix(college.test[9])
```

We then perform cross validation to find the best value for $\lambda$. 
```{r, fig.height=7, fig.width=9}
set.seed(1)
cv_outstate = cv.glmnet(college_predictors_train, college_response_train, alpha=1, nfolds=10)
cv_outstate
plot(cv_outstate)
```

The above plot shows that the value of $\lambda$ which minimizes the $\text{MSE}$ (shown as the leftmost vertical dotted line) does not remove any predictors from the model. While a low $\text{MSE}$ is desirable, so is a simplified model. The other value of $\lambda$ which is highlighted with a dotted line in the plot is `lambda.1se`. This value of $\lambda$ gives a slightly higher $\text{MSE}$, however, it only uses $9$ out of $17$ variables, thus greatly simplifying the model. 

However, we can see that allowing just one extra variable, giving a total of 10 predictors, the $\text{MSE}$ can be brought much closer to its minimum. We will use this model. 

```{r}
# Finding index of the last model to have 10 variables, meaning the one with the smallest MSE.
lambda_index = Position(function(x){x == 10}, cv_outstate$nzero, right=TRUE)
bestlambda = cv_outstate$lambda[lambda_index]
```

Now we find the test MSE obtained when using our chosen $\lambda$. 

```{r}
lasso_college = glmnet(college_predictors_train, college_response_train, lambda=bestlambda)
lasso_outstate_predictions = predict(lasso_college, s=bestlambda, newx=college_predictors_test)
mse_lasso = mean((lasso_outstate_predictions - college_response_test)^2)
```
The test MSE is `r mse_lasso`.

```{r}
coef(lasso_college)
```
The variables that were selected were `Private`, `Top10perc`, `Room.Board`, `Personal`, `PhD`, `Terminal`, `S.F.Ratio`, `perc.alumni`, `Expend` and `Grad.Rate`. 


