---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 31"
author: "Ingrid Sofie Skjetne and Johannes Voll Kolst√∏"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r rpackages,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(GGally)
library(corrplot)
library(e1071)
set.seed(1)
```

# Problem 4

## a)

Multiple choice questions:

i. True.
ii. True.
iii. True.
iv. True.



## b)
Loading the dataset.
```{r}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
```


Converting the response variable to a factor.
```{r}
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
diab_test_predictors = d.test[-1]
diab_test_responses = d.test[1]
```


Using cross-validation to find a good value of `cost` for a support vector classifier.
```{r}
costs = c(0.001, 0.01, 0.1, 1, 5, 10, 100)
cv_svc = tune(svm, diabetes ~ ., data=d.train, kernel="linear", ranges=list(cost=costs))
best_cost = cv_svc$best.parameters
```
Cross validation gave `r best_cost` as the optimal value of `cost`.

Fitting a support vector classifier. 
```{r}
diab_svc = svm(data=d.train, diabetes ~ ., kernel="linear", cost=best_cost, scale=FALSE)
```

 
Fitting a support vector machine with radial boundary. First using cross validation to find parameters $\gamma$ and cost. 
```{r}
gammas = 10^seq(-7, 1, length=9)
cv_svm = tune(svm, diabetes~., kernel="radial", ranges=list(cost=costs, gamma=gammas), data=d.train)

best_cost_svm = cv_svm$best.parameters[1]
best_gamma = cv_svm$best.parameters[2]
diab_svm = cv_svm$best.model
```

The best performance was achieved with a cost of `r best_cost_svm` and $\gamma=$ `r best_gamma`. 


Making predictions on the test set using the support vector classifier, and creating the confusion matrix.
```{r}
diab_svc_predictions = predict(diab_svc, diab_test_predictors)
table_df_svc = data.frame("prediction"= diab_svc_predictions, "actual"=d.test$diabetes)
confusion_svc = table("Predictions"=table_df_svc$prediction, "Actual"=table_df_svc$actual)
```

Making predictions on the test set using the support vector machine, and creating the confusion matrix.
```{r}
diab_svm_predictions = predict(diab_svm, diab_test_predictors)
table_df_svm = data.frame("prediction"= diab_svm_predictions, "actual"=d.test$diabetes)
confusion_svm = table("Predictions"=table_df_svm$prediction, "Actual"=table_df_svm$actual)
```

Get misclassification rates and print confusion matrices.
```{r, results="hold"}
misclass_svc = mean(table_df_svc$prediction != table_df_svc$actual)
misclass_svm = mean(table_df_svm$prediction != table_df_svm$actual)

confusion_svc
confusion_svm

```
The misclassification rates were `r misclass_svc` for the support vector classifier, and `r misclass_svm`for the support vector machine.

Finding the specificity and sensitivity of the two models when used on the test set. 

```{r, results="hold"}
sensitivity_svc = confusion_svc[2, 2]/(confusion_svc[1,2] + confusion_svc[2,2])
specificity_svc = confusion_svc[1, 1]/(confusion_svc[1,1] + confusion_svc[2,1])

sensitivity_svm = confusion_svm[2, 2]/(confusion_svm[1,2] + confusion_svm[2,2])
specificity_svm = confusion_svm[1, 1]/(confusion_svm[1,1] + confusion_svm[2,1])

sensitivity_svc
specificity_svc

sensitivity_svm
specificity_svm

```

The sensitivities of both models are quite low. If the models are to be used for prediction, one would want a high sensitivity considering that undiagnosed diabetes has negative consequences on a person's health. The support vector classfier, which has a linear kernel, performed slightly better than the support vector machine with a radial kernel.

The support vector machine had a higher specificity, however we would prefer using the support vector classifier.


## c)
Here we assume that a classifier which makes it possible to see which variables affect the chance of a woman having diabetes is preferred. We pick logistic regression, as it gives interpretable results, does not assume normal distribution or equal variances, and does not assume equal number of datapoints in each class. 

Fitting a logistic regression model to the training data.
```{r}
logreg_diab = glm(data=d.train, diabetes~., family="binomial")
```

Making predictions on the test set and creating the confusion table.
```{r}
logreg_pred = predict(logreg_diab, newdata=d.test, type="response")
logreg_classification = logreg_pred > 0.5

confusion_logreg = table("Prediction"=logreg_classification, "Actual"=d.test$diabetes==1)
confusion_logreg
```


Calculating sensitivity and specificity of logistic regression.
```{r, results="hold"}
sensitivity_logreg = confusion_logreg[2, 2]/(confusion_logreg[1,2] + confusion_logreg[2,2])
sensitivity_logreg

specificity_logreg = confusion_logreg[1, 1]/(confusion_logreg[1,1] + confusion_logreg[2,1])
specificity_logreg
```


Calculating misclassification rate.

```{r}
mean(logreg_classification*1 != d.test$diabetes)
```
Logistic regression has the advantage over SVMs that it gives an estimate of the probability of an observation belonging to a class, rather than just a predicted class. 

A disadvantage of logistic regression is that it is more sensitive to outliers. Support vector machines are not sensitive to outliers since these often end up not affecting the decision boundary since they are not among the support vectors.

Support vector machines are also at an advantage when the classes are well separated, as logistic regression runs into problems here. 


## d)
i. False.
ii. False.
iii. True.
iv. True.


## e)

The binomial deviance for an observation $i$, $D_i$ is defined as 
$$
D_i = -2\text{ log}\left(p_i^{I[y_i = 1]}(1- p_i)^{I[y_i = -1]}\right), \quad I[y_i = c] = \begin{cases} 1, \quad y_i = c,\\ 0, \quad \text{otherwise}.  \end{cases}
$$

where $p_i$ is the probability of observation $i$ belonging to the class $y_i = 1$, and we have coded the realizations of the random variable $Y_i$ as class $1$ if $y_i = 1$ and class $2$ if $y_i = -1$.  
For simplicity we will work with the scaled version $D_i/2 = -I[y_i=1]\text{ log}(p_i) - I[y_i = -1]\text{ log}(1 - p_i)$. 

In the logistic model, the probability $\text{Pr}(Y_i = 1) = p_i = \frac{\text{exp}(f(x_i))}{1 + \text{exp}(f(x_i))}$, 
and correspondingly $\text{Pr}(Y_i = -1) = 1 - p_i = \frac{1}{1 + \text{exp}(f(x_i))}$. 
In the regular logistic model, $f$ is linear in $x$ along with a constant term, giving $f(x) = \beta_0 + \beta^Tx$.

We find that 
\begin{align*}
\text{log}(p_i) &= f(x_i) - \text{log}(1 + \text{exp}(f(x_i)))\\
                &= f(x_i) - [\text{log}(1 + \text{exp}(-f(x_i))~) + f(x_i)]\\
                &= -\text{log}(1 + \text{exp}(-f(x_i))).
\end{align*}

and $\text{log}(1- p_i) = -\text{log}(1 + \text{exp}(f(x_i))~)$. 
Here we have used the identity $\text{log}(1 + \text{exp}(u)) = \text{log}(1 + \text{exp}(-u)) + u$.

Thus we can conclude that 
$$
D_i/2 = I[y_i=1]\text{ log}(1 + \text{exp}(f(x_i))~) + I[y_i=-1]\text{ log}(1 + \text{exp}(f(x_i))~) = \text{log}(1 + \text{exp}(-y_if(x_i))~).
$$
This last equation shows that the loss function $\text{log}(1 + \text{exp}(-y_if(x_i))~)$ represents a scaled deviance for the $y = -1, 1$ encoding of a logistic regression model.
