---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 31"
author: "Ingrid Sofie Skjetne and Johannes Voll Kolst√∏"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r rpackages,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(RColorBrewer)
library(splines)
library(caret)
```

# Problem 2

## a)
i) False
ii) False
iii) True
iv) True

## b)
The basis functions for a cubic spline with knots at the qaurtiles $q_1,~ q_2,~ q_3$ of variable $X$ are $\{1,~X,~X^2,~X^3,~h(X, q_1),~h(X, q_2),~h(X, q_3)\}$,
where the functions $h(X, q_i)$ are truncated third power functions of $X$, allowing for discontinuities in the third derivative at the knots. They are defined as
$$h(X, q_i) = 
\begin{cases}
(X - q_i)^3,&\quad q_i \leq X, \\
0,&\quad X \leq q_i.
\end{cases}
$$
In total we have $4 + K = 7$ degrees of freedom with these seven basis functions.

## c)

```{r task2c, eval=TRUE, echo=TRUE, fig.width=8, fig.height=10}
# Loading College data:
set.seed(1)
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
# str(College)

predictors = c("Private", "Room.Board", "Terminal", "perc.alumni", "Expend", "Grad.Rate")
response = c("Outstate")
College_2 = select(College, predictors, response)
names(College_2)

# Generating the six plots of the response Outstate versus the six predictors:
color_data = College_2$Private
p_Private = ggplot(College_2, aes(x = Private, y = Outstate, color=color_data)) + 
  geom_point() + theme(legend.position="top")
p_Room.Board = ggplot(College_2, aes(x = Room.Board, y = Outstate, color=color_data)) + 
  geom_point() + theme(legend.position="none")
p_Terminal = ggplot(College_2, aes(x = Terminal, y = Outstate, color=color_data)) + 
  geom_point() + theme(legend.position="none")
p_perc.alumni = ggplot(College_2, aes(x = perc.alumni, y = Outstate, color=color_data)) + 
  geom_point() + theme(legend.position="none") 
p_Expend = ggplot(College_2, aes(x = Expend, y = Outstate, color=color_data)) + 
  geom_point() + theme(legend.position="none")
p_Grad.Rate = ggplot(College_2, aes(x = Grad.Rate, y = Outstate, color=color_data)) + 
  geom_point() + theme(legend.position="none")

# Plotting the six plots in a 3X2-grid:
gridExtra::grid.arrange(p_Private, p_Room.Board, p_Terminal, p_perc.alumni, 
                        p_Expend, p_Grad.Rate, nrow=3, ncol=2)
```

By looking at the above plots of `Outstate` versus the six predictors `Private`, `Room.Board`, `Terminal`, `perc.alumni`, `Expend` and `Grad.Rate` there are several  interesting features present. The relationship between `Outstate` and `Terminal` seems reasonably linear and without need for a non-linear transformation, although we observe increasing variance with increasing predictor value. This also seems to be the case for the relationship between `Oustate` and the qualitative variable `Private`, as there appears to be an increase in the average `Outstate` for private colleges compared to public ones, but the variance in `Outstate` is also increased for the private colleges.  

The plot of `Outstate` versus `Grad.Rate` appears to have a clear linear relationship, and the variance appears more constant over the range of `Grad.Rate` values. In comparison, the relationship between `Expend` and `Outstate` appears linear until `Expend` increases above `20 000`, after which the `Outstate` values level off and vary greatly. In such a case a non-linear transformation of `Expend` seems beneficial for regression purposes, in order to avoid many the outliers with high leverage which would appear in a linear regression.

From the plot of `Outstate` versus `Terminal` it seems that there is a non-linear relationship between them. Especially the relationship between `Outstate` and the `Terminal` values for private colleges looks more quadratic than linear, and in need of a non-linear transformation. And even the public colleges appear to have a non-linear relationship between `Outstate` and `Terminal`, if not as clear.

Lastly, the relationship between `Outstate` and `perc.alumni` appears weak, and therefore an overly flexible non-linear basis function might be too flexible to give good predictive results. Therefore it does not appear beneficial to apply a non-linear transform to `per.alumni` before regression onto `Outstate`.


## d)
```{r task2d_1, eval=TRUE, echo=TRUE, fig.width=6, fig.height=5, fig.show="hold", results="hold"}

# Initializing storage for the polynomial regression models and predicted values:
poly_predicted_values = list()
poly_mse = list()

# Making the d_max polynomial regression models and find their prediction on a 
# sequence of Terminal values:
Terminal_range = range(College_2$Terminal)
Terminal_seq = seq(from=Terminal_range[1], to=Terminal_range[2], length.out = 200)
d_max = 10
for(i in 1:d_max){
  lm_i = lm(Outstate ~ poly(Terminal, i), data=College_2)
  poly_predicted_values[[i]] = predict(lm_i, newdata = list(Terminal=Terminal_seq))
  poly_mse[[i]] = sum(lm_i$residuals**2)/length(train.ind)
}

# Buil a dataframe out of the predicted values from the polynomial regression:
Outstate_fitted_df = do.call(cbind.data.frame, poly_predicted_values)
names(Outstate_fitted_df) = c("d1", "d2", "d3", "d4", "d5", "d6", "d7", "d8", "d9", "d10")
Outstate_fitted_df$Terminal_seq = Terminal_seq

# Plot a few of the polynomial fits along with the original data:
col = brewer.pal(n=8, name="Dark2")
poly_plot = ggplot(data = Outstate_fitted_df, aes(x = Terminal_seq)) + 
            geom_point(data=College_2, aes(x = Terminal, y=Outstate), color="blue") +
            xlab("Terminal")
str_degrees = names(Outstate_fitted_df)[1:10]
poly_plot = poly_plot +
      geom_line(aes(x = Terminal_seq, y = d1, colour="d1"), size=1.1) + 
      geom_line(aes(x = Terminal_seq, y = d2, colour="d2"), size=1.1) +
      geom_line(aes(x = Terminal_seq, y = d5, colour="d5"), size=1.1) +
      geom_line(aes(x = Terminal_seq, y = d7, colour="d7"), size=1.1) +
      geom_line(aes(x = Terminal_seq, y = d10, colour="d10"), size=1.1) +
      scale_colour_manual("",
                          breaks = c("d1", "d2", "d5", "d7", "d10"),
                          values = c("d1"=col[[1]], "d2"=col[[2]], "d5"=col[[3]], 
                                     "d7"=col[[4]], "d10"=col[[5]])) +
      labs(title="Polynomial Regression, Outstate ~ Terminal")
print(poly_plot)
```

```{r task2d_2, eval=TRUE, echo=TRUE, fig.width=6, fig.height=5, fig.show="hold", results="hold"}
### SPLINE REGRESSION ###
# Cross validate to find the best number of degrees of freedom for Spline-interpolation.
# Use cubic splines, with uniformly spaced knots.

# Performs LOOCV by setting cv=T:
smooth_spline_model = smooth.spline(x = College_2$Expend, y = College_2$Outstate, cv=T)
smooth_predict = predict(smooth_spline_model,College_2$Expend)

# length(College_2$Expend)
# length(smooth_predict$y)

smooth_df = do.call(data.frame, smooth_predict)
smooth_plot = ggplot(data=College_2, aes(x=Expend, y=Outstate)) + geom_point(size=1.1, col="blue") 
smooth_plot = smooth_plot +
              geom_line(data=smooth_df, aes(x=x, y=y, color="Smoothing spline"), size=1.1) +
              scale_color_manual("", values = c("Smoothing spline" = "red")) + 
              labs(title="Smoothing Splines, Outstate ~ Expend")
print(smooth_plot)
cat("Degrees of freedom for smoothing splines, chosen by LOOCV:", smooth_spline_model$df, "\n")

# Find the polynomial regression with the lowest MSE:
length(College_2$Outstate)
length(Outstate_fitted_df$d10)
min_poly_MSE = do.call(min, poly_mse)
smoothing_MSE = mean((College_2$Outstate - smooth_predict$y)^2)

cat("Degree 10 polynomail regression MSE:\n", min_poly_MSE, "\n")
cat("Smoothing spline regression MSE:\n", smoothing_MSE, "\n")
```
